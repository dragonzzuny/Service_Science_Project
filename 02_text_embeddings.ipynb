{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c716cf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기본 라이브러리 로드 완료\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 임베딩 생성\n",
    "\n",
    "## 1. 라이브러리 불러오기\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 시각화 설정\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "print(\"기본 라이브러리 로드 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8da8732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction_df 로드 완료\n",
      "train_df 로드 완료\n",
      "val_df 로드 완료\n",
      "test_df 로드 완료\n",
      "train_with_neg 로드 완료\n",
      "user_processed 로드 완료\n",
      "business_processed 로드 완료\n",
      "tip_processed 로드 완료\n",
      "user_to_idx 로드 완료\n",
      "idx_to_user 로드 완료\n",
      "business_to_idx 로드 완료\n",
      "idx_to_business 로드 완료\n",
      "훈련 세트 크기: 20,674\n",
      "검증 세트 크기: 4,128\n",
      "테스트 세트 크기: 1,748\n",
      "리뷰 텍스트 샘플: good food loved the gnocchi with marinara the baked eggplant appetizer was very good too the service...\n"
     ]
    }
   ],
   "source": [
    "## 2. 전처리된 데이터 로드\n",
    "\n",
    "# 데이터 경로 설정\n",
    "PROCESSED_DIR = \"./data/processed\"\n",
    "EMBEDDINGS_DIR = \"./data/embeddings\"\n",
    "\n",
    "# 디렉토리 생성\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "\n",
    "# 필요한 데이터 파일 로드\n",
    "data_files = {\n",
    "    'interaction_df': 'interaction_df.pkl',\n",
    "    'train_df': 'train_df.pkl',\n",
    "    'val_df': 'val_df.pkl',\n",
    "    'test_df': 'test_df.pkl',\n",
    "    'train_with_neg': 'train_with_neg.pkl',\n",
    "    'user_processed': 'user_processed.pkl',\n",
    "    'business_processed': 'business_processed.pkl',\n",
    "    'tip_processed': 'tip_processed.pkl',\n",
    "    'user_to_idx': 'user_to_idx.pkl',\n",
    "    'idx_to_user': 'idx_to_user.pkl',\n",
    "    'business_to_idx': 'business_to_idx.pkl',\n",
    "    'idx_to_business': 'idx_to_business.pkl'\n",
    "}\n",
    "\n",
    "# 데이터 로드\n",
    "data = {}\n",
    "for name, file in data_files.items():\n",
    "    with open(os.path.join(PROCESSED_DIR, file), 'rb') as f:\n",
    "        data[name] = pickle.load(f)\n",
    "    print(f\"{name} 로드 완료\")\n",
    "\n",
    "# 데이터 변수에 할당\n",
    "interaction_df = data['interaction_df']\n",
    "train_df = data['train_df']\n",
    "val_df = data['val_df']\n",
    "test_df = data['test_df']\n",
    "train_with_neg = data['train_with_neg']\n",
    "user_processed = data['user_processed']\n",
    "business_processed = data['business_processed']\n",
    "tip_processed = data['tip_processed']\n",
    "user_to_idx = data['user_to_idx']\n",
    "idx_to_user = data['idx_to_user']\n",
    "business_to_idx = data['business_to_idx']\n",
    "idx_to_business = data['idx_to_business']\n",
    "\n",
    "print(f\"훈련 세트 크기: {len(train_df):,}\")\n",
    "print(f\"검증 세트 크기: {len(val_df):,}\")\n",
    "print(f\"테스트 세트 크기: {len(test_df):,}\")\n",
    "print(f\"리뷰 텍스트 샘플: {interaction_df['clean_text'].iloc[0][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f23ab05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 타입: bert\n",
      "최대 길이: 256\n",
      "임베딩 차원: 768\n",
      "차원 축소 후 차원: 256\n",
      "GPU 사용 가능: True\n"
     ]
    }
   ],
   "source": [
    "## 3. 텍스트 임베딩 방식 설정\n",
    "\n",
    "# 설정값\n",
    "EMBEDDING_TYPE = \"bert\"  # 또는 \"word2vec\"\n",
    "MAX_LEN = 256  # 시퀀스 최대 길이\n",
    "EMBED_DIM = 768  # BERT 기본 임베딩 차원\n",
    "REDUCED_DIM = 256  # 차원 축소 후 차원 (계산 효율성을 위해)\n",
    "BATCH_SIZE = 16  # 배치 크기\n",
    "USE_GPU = torch.cuda.is_available()  # GPU 사용 여부\n",
    "\n",
    "print(f\"임베딩 타입: {EMBEDDING_TYPE}\")\n",
    "print(f\"최대 길이: {MAX_LEN}\")\n",
    "print(f\"임베딩 차원: {EMBED_DIM}\")\n",
    "print(f\"차원 축소 후 차원: {REDUCED_DIM}\")\n",
    "print(f\"GPU 사용 가능: {USE_GPU}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4bd0978e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers 라이브러리를 설치합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PYJ\\anaconda3\\envs\\lab\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers 라이브러리 설치 완료.\n"
     ]
    }
   ],
   "source": [
    "## 4. BERT 임베딩 함수 구현\n",
    "\n",
    "# BERT 관련 라이브러리 설치 및 임포트\n",
    "try:\n",
    "    from transformers import BertTokenizer, BertModel\n",
    "    print(\"Transformers 라이브러리가 이미 설치되어 있습니다.\")\n",
    "except ImportError:\n",
    "    print(\"Transformers 라이브러리를 설치합니다...\")\n",
    "    !pip install -q transformers\n",
    "    from transformers import BertTokenizer, BertModel\n",
    "    print(\"Transformers 라이브러리 설치 완료.\")\n",
    "\n",
    "def create_bert_embeddings(texts, batch_size=16, max_length=256, model_name=\"bert-base-uncased\", device=None):\n",
    "    \"\"\"\n",
    "    텍스트 리스트를 BERT 임베딩으로 변환\n",
    "    \n",
    "    Args:\n",
    "        texts (list): 임베딩할 텍스트 리스트\n",
    "        batch_size (int): 배치 크기\n",
    "        max_length (int): 최대 토큰 길이\n",
    "        model_name (str): 사용할 BERT 모델 이름\n",
    "        device (str): 사용할 장치 ('cuda' 또는 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 텍스트 임베딩 (texts_count x embed_dim)\n",
    "    \"\"\"\n",
    "    # 장치 설정\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # 토크나이저 및 모델 로드\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # 결과 저장을 위한 리스트\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # 배치 단위로 처리\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"BERT 임베딩 생성\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # 빈 텍스트 처리\n",
    "        batch_texts = [text if isinstance(text, str) and text.strip() else \"\" for text in batch_texts]\n",
    "        \n",
    "        # 토큰화\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ).to(device)\n",
    "        \n",
    "        # 임베딩 생성\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # [CLS] 토큰의 마지막 히든 스테이트 사용 (문장 전체 표현)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_embeddings.append(embeddings)\n",
    "    \n",
    "    # 모든 임베딩 결합\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    return all_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21e872a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGensim 및 NLTK 라이브러리가 이미 설치되어 있습니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PYJ\\anaconda3\\envs\\lab\\lib\\site-packages\\gensim\\__init__.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4.3.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logger\u001b[38;5;241m.\u001b[39mhandlers:  \u001b[38;5;66;03m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PYJ\\anaconda3\\envs\\lab\\lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexedcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IndexedCorpus  \u001b[38;5;66;03m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmmcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MmCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbleicorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BleiCorpus  \u001b[38;5;66;03m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PYJ\\anaconda3\\envs\\lab\\lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     16\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIndexedCorpus\u001b[39;00m(interfaces\u001b[38;5;241m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32mc:\\Users\\PYJ\\anaconda3\\envs\\lab\\lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCorpusABC\u001b[39;00m(utils\u001b[38;5;241m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32mc:\\Users\\PYJ\\anaconda3\\envs\\lab\\lib\\site-packages\\gensim\\matutils.py:1034\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mlen\u001b[39m(set1 \u001b[38;5;241m&\u001b[39m set2)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(union_cardinality)\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1033\u001b[0m     \u001b[38;5;66;03m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[1;32m-> 1034\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_matutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlogsumexp\u001b[39m(x):\n",
      "File \u001b[1;32mc:\\Users\\PYJ\\anaconda3\\envs\\lab\\lib\\site-packages\\gensim\\_matutils.pyx:1\u001b[0m, in \u001b[0;36minit gensim._matutils\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "## 5. Word2Vec 임베딩 함수 구현 (대체 접근법)\n",
    "\n",
    "# Word2Vec 관련 라이브러리 설치 및 임포트\n",
    "try:\n",
    "    import nltk\n",
    "    from gensim.models import Word2Vec\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    print(\"Gensim 및 NLTK 라이브러리가 이미 설치되어 있습니다.\")\n",
    "except ImportError:\n",
    "    print(\"Gensim 및 NLTK 라이브러리를 설치합니다...\")\n",
    "    !pip install -q gensim nltk\n",
    "    import nltk\n",
    "    from gensim.models import Word2Vec\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    print(\"Gensim 및 NLTK 라이브러리 설치 완료.\")\n",
    "    \n",
    "# NLTK 데이터 다운로드\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"NLTK punkt 토크나이저가 이미 설치되어 있습니다.\")\n",
    "except LookupError:\n",
    "    print(\"NLTK punkt 토크나이저를 다운로드합니다...\")\n",
    "    nltk.download('punkt')\n",
    "    print(\"NLTK punkt 토크나이저 다운로드 완료.\")\n",
    "\n",
    "def create_word2vec_embeddings(texts, vector_size=100, window=5, min_count=1):\n",
    "    \"\"\"\n",
    "    텍스트 리스트를 Word2Vec 임베딩으로 변환\n",
    "    \n",
    "    Args:\n",
    "        texts (list): 임베딩할 텍스트 리스트\n",
    "        vector_size (int): 벡터 크기\n",
    "        window (int): 컨텍스트 윈도우 크기\n",
    "        min_count (int): 최소 단어 빈도\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (텍스트 임베딩 배열, Word2Vec 모델)\n",
    "    \"\"\"\n",
    "    # 텍스트 토큰화\n",
    "    tokenized_texts = []\n",
    "    for text in tqdm(texts, desc=\"텍스트 토큰화\"):\n",
    "        if isinstance(text, str) and text.strip():\n",
    "            tokenized_texts.append(word_tokenize(text.lower()))\n",
    "        else:\n",
    "            tokenized_texts.append([])\n",
    "    \n",
    "    # Word2Vec 모델 학습\n",
    "    print(\"Word2Vec 모델 학습 중...\")\n",
    "    model = Word2Vec(\n",
    "        sentences=tokenized_texts,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=4\n",
    "    )\n",
    "    print(\"Word2Vec 모델 학습 완료.\")\n",
    "    \n",
    "    # 텍스트별 임베딩 생성 (단어 벡터의 평균)\n",
    "    embeddings = []\n",
    "    for tokens in tqdm(tokenized_texts, desc=\"텍스트 임베딩 생성\"):\n",
    "        if tokens:\n",
    "            # 모델에 있는 토큰만 사용\n",
    "            valid_tokens = [token for token in tokens if token in model.wv]\n",
    "            if valid_tokens:\n",
    "                # 단어 벡터의 평균 계산\n",
    "                embedding = np.mean([model.wv[token] for token in valid_tokens], axis=0)\n",
    "            else:\n",
    "                # 유효한 토큰이 없으면 0 벡터 사용\n",
    "                embedding = np.zeros(vector_size)\n",
    "        else:\n",
    "            # 빈 텍스트는 0 벡터 사용\n",
    "            embedding = np.zeros(vector_size)\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    return np.array(embeddings), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31cd7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. 리뷰 텍스트 임베딩 생성\n",
    "\n",
    "# 훈련 세트의 리뷰 텍스트 임베딩 생성\n",
    "if EMBEDDING_TYPE == \"bert\":\n",
    "    print(\"BERT 임베딩 생성 중...\")\n",
    "    train_review_embeddings = create_bert_embeddings(\n",
    "        train_df['clean_text'].tolist(),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_length=MAX_LEN,\n",
    "        device='cuda' if USE_GPU else 'cpu'\n",
    "    )\n",
    "    print(f\"훈련 리뷰 임베딩 크기: {train_review_embeddings.shape}\")\n",
    "    \n",
    "    # 검증 세트의 리뷰 텍스트 임베딩 생성\n",
    "    val_review_embeddings = create_bert_embeddings(\n",
    "        val_df['clean_text'].tolist(),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_length=MAX_LEN,\n",
    "        device='cuda' if USE_GPU else 'cpu'\n",
    "    )\n",
    "    print(f\"검증 리뷰 임베딩 크기: {val_review_embeddings.shape}\")\n",
    "    \n",
    "    # 테스트 세트의 리뷰 텍스트 임베딩 생성\n",
    "    test_review_embeddings = create_bert_embeddings(\n",
    "        test_df['clean_text'].tolist(),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_length=MAX_LEN,\n",
    "        device='cuda' if USE_GPU else 'cpu'\n",
    "    )\n",
    "    print(f\"테스트 리뷰 임베딩 크기: {test_review_embeddings.shape}\")\n",
    "else:\n",
    "    print(\"Word2Vec 임베딩 생성 중...\")\n",
    "    # 모든 훈련 텍스트로 Word2Vec 모델 학습\n",
    "    all_texts = train_df['clean_text'].tolist() + val_df['clean_text'].tolist() + test_df['clean_text'].tolist()\n",
    "    all_embeddings, word2vec_model = create_word2vec_embeddings(\n",
    "        all_texts,\n",
    "        vector_size=100\n",
    "    )\n",
    "    \n",
    "    # 세트별로 임베딩 분리\n",
    "    train_size = len(train_df)\n",
    "    val_size = len(val_df)\n",
    "    test_size = len(test_df)\n",
    "    \n",
    "    train_review_embeddings = all_embeddings[:train_size]\n",
    "    val_review_embeddings = all_embeddings[train_size:train_size+val_size]\n",
    "    test_review_embeddings = all_embeddings[train_size+val_size:]\n",
    "    \n",
    "    print(f\"훈련 리뷰 임베딩 크기: {train_review_embeddings.shape}\")\n",
    "    print(f\"검증 리뷰 임베딩 크기: {val_review_embeddings.shape}\")\n",
    "    print(f\"테스트 리뷰 임베딩 크기: {test_review_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e99830",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. 팁 텍스트 임베딩 생성 (추가 데이터)\n",
    "\n",
    "# 팁 텍스트 임베딩 생성\n",
    "if len(tip_processed) > 0:\n",
    "    if EMBEDDING_TYPE == \"bert\":\n",
    "        print(\"팁 BERT 임베딩 생성 중...\")\n",
    "        tip_embeddings = create_bert_embeddings(\n",
    "            tip_processed['clean_text'].tolist(),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            max_length=MAX_LEN,\n",
    "            device='cuda' if USE_GPU else 'cpu'\n",
    "        )\n",
    "    else:\n",
    "        print(\"팁 Word2Vec 임베딩 생성 중...\")\n",
    "        tip_embeddings, _ = create_word2vec_embeddings(\n",
    "            tip_processed['clean_text'].tolist(),\n",
    "            vector_size=100\n",
    "        )\n",
    "    \n",
    "    print(f\"팁 임베딩 크기: {tip_embeddings.shape}\")\n",
    "else:\n",
    "    print(\"팁 데이터가 없거나 비어 있습니다.\")\n",
    "    tip_embeddings = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. 차원 축소 적용 (선택적)\n",
    "\n",
    "def reduce_dimensions(embeddings, method='pca', n_components=256, random_state=42):\n",
    "    \"\"\"\n",
    "    고차원 임베딩을 저차원으로 축소\n",
    "    \n",
    "    Args:\n",
    "        embeddings (np.ndarray): 임베딩 배열\n",
    "        method (str): 차원 축소 방법 ('pca' 또는 'tsne')\n",
    "        n_components (int): 축소 후 차원 수\n",
    "        random_state (int): 랜덤 시드\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 축소된 임베딩\n",
    "    \"\"\"\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components, random_state=random_state)\n",
    "    elif method == 'tsne':\n",
    "        reducer = TSNE(n_components=n_components, random_state=random_state)\n",
    "    else:\n",
    "        raise ValueError(\"지원되지 않는 차원 축소 방법입니다. 'pca' 또는 'tsne'를 사용하세요.\")\n",
    "    \n",
    "    print(f\"{method.upper()} 차원 축소 적용 중 ({embeddings.shape[1]} -> {n_components})...\")\n",
    "    reduced_embeddings = reducer.fit_transform(embeddings)\n",
    "    print(f\"차원 축소 완료: {reduced_embeddings.shape}\")\n",
    "    \n",
    "    if method == 'pca':\n",
    "        # 설명된 분산 비율 출력\n",
    "        explained_variance = reducer.explained_variance_ratio_.sum()\n",
    "        print(f\"설명된 분산 비율: {explained_variance:.4f}\")\n",
    "    \n",
    "    return reduced_embeddings\n",
    "\n",
    "# BERT 임베딩인 경우 차원 축소 적용\n",
    "if EMBEDDING_TYPE == \"bert\" and REDUCED_DIM < EMBED_DIM:\n",
    "    # 훈련 세트 리뷰 임베딩 차원 축소\n",
    "    train_review_embeddings_reduced = reduce_dimensions(\n",
    "        train_review_embeddings,\n",
    "        method='pca',\n",
    "        n_components=REDUCED_DIM\n",
    "    )\n",
    "    \n",
    "    # 검증 세트 리뷰 임베딩 차원 축소\n",
    "    val_review_embeddings_reduced = reduce_dimensions(\n",
    "        val_review_embeddings,\n",
    "        method='pca',\n",
    "        n_components=REDUCED_DIM\n",
    "    )\n",
    "    \n",
    "    # 테스트 세트 리뷰 임베딩 차원 축소\n",
    "    test_review_embeddings_reduced = reduce_dimensions(\n",
    "        test_review_embeddings,\n",
    "        method='pca',\n",
    "        n_components=REDUCED_DIM\n",
    "    )\n",
    "    \n",
    "    # 팁 임베딩 차원 축소\n",
    "    if len(tip_embeddings) > 0:\n",
    "        tip_embeddings_reduced = reduce_dimensions(\n",
    "            tip_embeddings,\n",
    "            method='pca',\n",
    "            n_components=REDUCED_DIM\n",
    "        )\n",
    "    else:\n",
    "        tip_embeddings_reduced = np.array([])\n",
    "else:\n",
    "    # 차원 축소를 적용하지 않는 경우 원본 임베딩 사용\n",
    "    train_review_embeddings_reduced = train_review_embeddings\n",
    "    val_review_embeddings_reduced = val_review_embeddings\n",
    "    test_review_embeddings_reduced = test_review_embeddings\n",
    "    tip_embeddings_reduced = tip_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead322c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. 임베딩 시각화 (t-SNE)\n",
    "\n",
    "def visualize_embeddings(embeddings, labels, n_samples=1000, perplexity=30, title=\"임베딩 시각화\"):\n",
    "    \"\"\"\n",
    "    t-SNE를 사용하여 임베딩 시각화\n",
    "    \n",
    "    Args:\n",
    "        embeddings (np.ndarray): 임베딩 배열\n",
    "        labels (pd.Series): 레이블 (예: 평점, 카테고리 등)\n",
    "        n_samples (int): 시각화할 샘플 수\n",
    "        perplexity (int): t-SNE 매개변수\n",
    "        title (str): 시각화 제목\n",
    "    \"\"\"\n",
    "    # 샘플 선택\n",
    "    if len(embeddings) > n_samples:\n",
    "        indices = np.random.choice(len(embeddings), n_samples, replace=False)\n",
    "        embeddings_sample = embeddings[indices]\n",
    "        labels_sample = labels.iloc[indices] if isinstance(labels, pd.Series) else labels[indices]\n",
    "    else:\n",
    "        embeddings_sample = embeddings\n",
    "        labels_sample = labels\n",
    "    \n",
    "    # t-SNE 적용\n",
    "    print(f\"t-SNE 적용 중 (perplexity={perplexity})...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings_sample)\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # 레이블이 숫자(평점)인 경우 스캐터 플롯 사용\n",
    "    if np.issubdtype(np.array(labels_sample).dtype, np.number):\n",
    "        scatter = plt.scatter(\n",
    "            embeddings_2d[:, 0], \n",
    "            embeddings_2d[:, 1], \n",
    "            c=labels_sample, \n",
    "            cmap='viridis', \n",
    "            alpha=0.6,\n",
    "            s=50\n",
    "        )\n",
    "        plt.colorbar(scatter, label='평점')\n",
    "    # 레이블이 카테고리인 경우 다른 색상으로 표시\n",
    "    else:\n",
    "        unique_labels = np.unique(labels_sample)\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n",
    "        \n",
    "        for i, label in enumerate(unique_labels):\n",
    "            mask = labels_sample == label\n",
    "            plt.scatter(\n",
    "                embeddings_2d[mask, 0], \n",
    "                embeddings_2d[mask, 1], \n",
    "                color=colors[i], \n",
    "                label=label,\n",
    "                alpha=0.6,\n",
    "                s=50\n",
    "            )\n",
    "        plt.legend(title='카테고리')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# 임베딩 시각화 (평점별)\n",
    "visualize_embeddings(\n",
    "    train_review_embeddings_reduced,\n",
    "    train_df['rating'],\n",
    "    n_samples=2000,\n",
    "    perplexity=30,\n",
    "    title=\"리뷰 임베딩 t-SNE 시각화 (평점별)\"\n",
    ")\n",
    "\n",
    "# 상위 아이템 카테고리 추출\n",
    "top_items = train_df['item_idx'].value_counts().head(5).index.tolist()\n",
    "top_items_mask = train_df['item_idx'].isin(top_items)\n",
    "top_items_embeddings = train_review_embeddings_reduced[top_items_mask]\n",
    "top_items_labels = train_df.loc[top_items_mask, 'item_idx']\n",
    "\n",
    "# 인기 아이템별 시각화\n",
    "visualize_embeddings(\n",
    "    top_items_embeddings,\n",
    "    top_items_labels,\n",
    "    perplexity=15,\n",
    "    title=\"리뷰 임베딩 t-SNE 시각화 (인기 아이템별)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. 아이템별 임베딩 생성 (리뷰 임베딩 평균)\n",
    "\n",
    "def create_item_embeddings(interaction_df, review_embeddings, item_ids):\n",
    "    \"\"\"\n",
    "    아이템별 임베딩 생성 (해당 아이템의 모든 리뷰 임베딩 평균)\n",
    "    \n",
    "    Args:\n",
    "        interaction_df (pd.DataFrame): 상호작용 데이터프레임\n",
    "        review_embeddings (np.ndarray): 리뷰 임베딩 배열\n",
    "        item_ids (list): 아이템 ID 리스트\n",
    "        \n",
    "    Returns:\n",
    "        dict: 아이템 ID를 키로, 임베딩 벡터를 값으로 하는 딕셔너리\n",
    "    \"\"\"\n",
    "    item_embeddings = {}\n",
    "    \n",
    "    for item_idx in tqdm(item_ids, desc=\"아이템별 임베딩 생성\"):\n",
    "        # 해당 아이템의 리뷰 인덱스 찾기\n",
    "        item_review_indices = interaction_df[interaction_df['item_idx'] == item_idx].index\n",
    "        \n",
    "        if len(item_review_indices) > 0:\n",
    "            # 해당 아이템의 리뷰 임베딩 추출\n",
    "            item_review_embeds = review_embeddings[item_review_indices]\n",
    "            \n",
    "            # 평균 임베딩 계산\n",
    "            item_embedding = np.mean(item_review_embeds, axis=0)\n",
    "        else:\n",
    "            # 리뷰가 없는 경우 0 벡터 사용\n",
    "            item_embedding = np.zeros(review_embeddings.shape[1])\n",
    "        \n",
    "        # 딕셔너리에 저장\n",
    "        item_embeddings[item_idx] = item_embedding\n",
    "    \n",
    "    return item_embeddings\n",
    "\n",
    "# 훈련 세트 아이템 임베딩 생성\n",
    "train_items = train_df['item_idx'].unique()\n",
    "train_item_embeddings = create_item_embeddings(train_df, train_review_embeddings_reduced, train_items)\n",
    "print(f\"훈련 아이템 임베딩 수: {len(train_item_embeddings)}\")\n",
    "\n",
    "# 임베딩 배열 형태로 변환\n",
    "train_item_embeds_array = np.array([train_item_embeddings[idx] for idx in sorted(train_item_embeddings.keys())])\n",
    "print(f\"훈련 아이템 임베딩 배열 크기: {train_item_embeds_array.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f981bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. 사용자별 임베딩 생성 (사용자 리뷰 임베딩 평균)\n",
    "\n",
    "def create_user_embeddings(interaction_df, review_embeddings, user_ids):\n",
    "    \"\"\"\n",
    "    사용자별 임베딩 생성 (사용자가 작성한 모든 리뷰 임베딩 평균)\n",
    "    \n",
    "    Args:\n",
    "        interaction_df (pd.DataFrame): 상호작용 데이터프레임\n",
    "        review_embeddings (np.ndarray): 리뷰 임베딩 배열\n",
    "        user_ids (list): 사용자 ID 리스트\n",
    "        \n",
    "    Returns:\n",
    "        dict: 사용자 ID를 키로, 임베딩 벡터를 값으로 하는 딕셔너리\n",
    "    \"\"\"\n",
    "    user_embeddings = {}\n",
    "    \n",
    "    for user_idx in tqdm(user_ids, desc=\"사용자별 임베딩 생성\"):\n",
    "        # 해당 사용자의 리뷰 인덱스 찾기\n",
    "        user_review_indices = interaction_df[interaction_df['user_idx'] == user_idx].index\n",
    "        \n",
    "        if len(user_review_indices) > 0:\n",
    "            # 해당 사용자의 리뷰 임베딩 추출\n",
    "            user_review_embeds = review_embeddings[user_review_indices]\n",
    "            \n",
    "            # 평균 임베딩 계산\n",
    "            user_embedding = np.mean(user_review_embeds, axis=0)\n",
    "        else:\n",
    "            # 리뷰가 없는 경우 0 벡터 사용\n",
    "            user_embedding = np.zeros(review_embeddings.shape[1])\n",
    "        \n",
    "        # 딕셔너리에 저장\n",
    "        user_embeddings[user_idx] = user_embedding\n",
    "    \n",
    "    return user_embeddings\n",
    "\n",
    "# 훈련 세트 사용자 임베딩 생성\n",
    "train_users = train_df['user_idx'].unique()\n",
    "train_user_embeddings = create_user_embeddings(train_df, train_review_embeddings_reduced, train_users)\n",
    "print(f\"훈련 사용자 임베딩 수: {len(train_user_embeddings)}\")\n",
    "\n",
    "# 임베딩 배열 형태로 변환\n",
    "train_user_embeds_array = np.array([train_user_embeddings[idx] for idx in sorted(train_user_embeddings.keys())])\n",
    "print(f\"훈련 사용자 임베딩 배열 크기: {train_user_embeds_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d987aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 12. 팁 데이터를 활용한 임베딩 보강 (선택적)\n",
    "\n",
    "def enhance_item_embeddings_with_tips(item_embeddings, tip_df, tip_embeddings, weight=0.3):\n",
    "    \"\"\"\n",
    "    팁 데이터를 활용하여 아이템 임베딩 보강\n",
    "    \n",
    "    Args:\n",
    "        item_embeddings (dict): 아이템 임베딩 딕셔너리\n",
    "        tip_df (pd.DataFrame): 팁 데이터프레임\n",
    "        tip_embeddings (np.ndarray): 팁 임베딩 배열\n",
    "        weight (float): 팁 임베딩 가중치 (0-1 사이)\n",
    "        \n",
    "    Returns:\n",
    "        dict: 보강된 아이템 임베딩 딕셔너리\n",
    "    \"\"\"\n",
    "    enhanced_embeddings = item_embeddings.copy()\n",
    "    \n",
    "    for item_idx in tqdm(item_embeddings.keys(), desc=\"팁 임베딩으로 보강\"):\n",
    "        # 해당 아이템의 팁 인덱스 찾기\n",
    "        item_tip_indices = tip_df[tip_df['item_idx'] == item_idx].index\n",
    "        \n",
    "        if len(item_tip_indices) > 0:\n",
    "            # 해당 아이템의 팁 임베딩 추출\n",
    "            item_tip_embeds = tip_embeddings[item_tip_indices]\n",
    "            \n",
    "            # 평균 팁 임베딩 계산\n",
    "            item_tip_embedding = np.mean(item_tip_embeds, axis=0)\n",
    "            \n",
    "            # 기존 리뷰 임베딩과 팁 임베딩을 가중 결합\n",
    "            original_embedding = enhanced_embeddings[item_idx]\n",
    "            enhanced_embeddings[item_idx] = (1 - weight) * original_embedding + weight * item_tip_embedding\n",
    "    \n",
    "    return enhanced_embeddings\n",
    "\n",
    "# 팁 데이터를 활용한 임베딩 보강 (팁 데이터가 있는 경우)\n",
    "if len(tip_processed) > 0 and len(tip_embeddings_reduced) > 0:\n",
    "    enhanced_item_embeddings = enhance_item_embeddings_with_tips(\n",
    "        train_item_embeddings,\n",
    "        tip_processed,\n",
    "        tip_embeddings_reduced,\n",
    "        weight=0.3\n",
    "    )\n",
    "    print(f\"팁으로 보강된 아이템 임베딩 수: {len(enhanced_item_embeddings)}\")\n",
    "    \n",
    "    # 임베딩 배열 형태로 변환\n",
    "    enhanced_item_embeds_array = np.array([enhanced_item_embeddings[idx] for idx in sorted(enhanced_item_embeddings.keys())])\n",
    "    print(f\"보강된 아이템 임베딩 배열 크기: {enhanced_item_embeds_array.shape}\")\n",
    "else:\n",
    "    print(\"팁 데이터가 없어 임베딩 보강을 건너뜁니다.\")\n",
    "    enhanced_item_embeddings = train_item_embeddings\n",
    "    enhanced_item_embeds_array = train_item_embeds_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3745f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 13. 검증 및 테스트 세트의 아이템 및 사용자 임베딩 생성\n",
    "\n",
    "# 검증 세트 아이템 및 사용자 임베딩 생성\n",
    "val_items = val_df['item_idx'].unique()\n",
    "val_users = val_df['user_idx'].unique()\n",
    "\n",
    "val_item_embeddings = create_item_embeddings(val_df, val_review_embeddings_reduced, val_items)\n",
    "val_user_embeddings = create_user_embeddings(val_df, val_review_embeddings_reduced, val_users)\n",
    "\n",
    "print(f\"검증 아이템 임베딩 수: {len(val_item_embeddings)}\")\n",
    "print(f\"검증 사용자 임베딩 수: {len(val_user_embeddings)}\")\n",
    "\n",
    "# 테스트 세트 아이템 및 사용자 임베딩 생성\n",
    "test_items = test_df['item_idx'].unique()\n",
    "test_users = test_df['user_idx'].unique()\n",
    "\n",
    "test_item_embeddings = create_item_embeddings(test_df, test_review_embeddings_reduced, test_items)\n",
    "test_user_embeddings = create_user_embeddings(test_df, test_review_embeddings_reduced, test_users)\n",
    "\n",
    "print(f\"테스트 아이템 임베딩 수: {len(test_item_embeddings)}\")\n",
    "print(f\"테스트 사용자 임베딩 수: {len(test_user_embeddings)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 14. 임베딩 저장\n",
    "\n",
    "# 임베딩 저장\n",
    "embeddings_to_save = {\n",
    "    'train_review_embeddings': train_review_embeddings_reduced,\n",
    "    'val_review_embeddings': val_review_embeddings_reduced,\n",
    "    'test_review_embeddings': test_review_embeddings_reduced,\n",
    "    'train_item_embeddings': train_item_embeddings,\n",
    "    'val_item_embeddings': val_item_embeddings,\n",
    "    'test_item_embeddings': test_item_embeddings,\n",
    "    'train_user_embeddings': train_user_embeddings,\n",
    "    'val_user_embeddings': val_user_embeddings,\n",
    "    'test_user_embeddings': test_user_embeddings,\n",
    "    'enhanced_item_embeddings': enhanced_item_embeddings\n",
    "}\n",
    "\n",
    "if len(tip_embeddings_reduced) > 0:\n",
    "    embeddings_to_save['tip_embeddings'] = tip_embeddings_reduced\n",
    "\n",
    "for name, embedding in embeddings_to_save.items():\n",
    "    file_path = os.path.join(EMBEDDINGS_DIR, f\"{name}.pkl\")\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(embedding, f)\n",
    "    print(f\"{name} 저장 완료: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fbf4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 15. 임베딩 분석\n",
    "\n",
    "# 아이템 임베딩 간 유사도 계산 (코사인 유사도)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_similar_items(item_idx, item_embeddings, business_data, top_n=5):\n",
    "    \"\"\"\n",
    "    임베딩 유사도를 기반으로 유사한 아이템 찾기\n",
    "    \n",
    "    Args:\n",
    "        item_idx (int): 기준 아이템 인덱스\n",
    "        item_embeddings (dict): 아이템 임베딩 딕셔너리\n",
    "        business_data (pd.DataFrame): 비즈니스 데이터프레임\n",
    "        top_n (int): 반환할 유사 아이템 수\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: 유사한 아이템 데이터프레임\n",
    "    \"\"\"\n",
    "    if item_idx not in item_embeddings:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # 기준 아이템 임베딩\n",
    "    query_embedding = item_embeddings[item_idx].reshape(1, -1)\n",
    "    \n",
    "    # 모든 아이템 ID와 임베딩\n",
    "    all_item_ids = list(item_embeddings.keys())\n",
    "    all_embeddings = np.array([item_embeddings[idx] for idx in all_item_ids])\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    similarities = cosine_similarity(query_embedding, all_embeddings).flatten()\n",
    "    \n",
    "    # 유사도 기준 상위 아이템 인덱스 (자기 자신 제외)\n",
    "    top_indices = np.argsort(similarities)[::-1][1:top_n+1]\n",
    "    \n",
    "    # 결과 데이터프레임 생성\n",
    "    similar_items = []\n",
    "    for idx in top_indices:\n",
    "        similar_item_idx = all_item_ids[idx]\n",
    "        similar_items.append({\n",
    "            'item_idx': similar_item_idx,\n",
    "            'name': business_data.loc[business_data['item_idx'] == similar_item_idx, 'name'].iloc[0],\n",
    "            'stars': business_data.loc[business_data['item_idx'] == similar_item_idx, 'stars'].iloc[0],\n",
    "            'similarity': similarities[idx]\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(similar_items)\n",
    "\n",
    "# 인기 아이템에 대한 유사 아이템 찾기\n",
    "popular_items = business_processed['item_idx'].value_counts().head(3).index.tolist()\n",
    "\n",
    "for item_idx in popular_items:\n",
    "    business_name = business_processed.loc[business_processed['item_idx'] == item_idx, 'name'].iloc[0]\n",
    "    print(f\"\\n=== '{business_name}' 과 유사한 비즈니스 ===\")\n",
    "    \n",
    "    similar_items = find_similar_items(\n",
    "        item_idx, \n",
    "        enhanced_item_embeddings, \n",
    "        business_processed,\n",
    "        top_n=5\n",
    "    )\n",
    "    \n",
    "    print(similar_items[['name', 'stars', 'similarity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e898645",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 16. 임베딩 품질 평가\n",
    "\n",
    "# 임베딩 품질 평가 (같은 평점의 리뷰가 서로 더 유사한지 확인)\n",
    "def evaluate_embedding_quality(review_embeddings, ratings, n_samples=1000):\n",
    "    \"\"\"\n",
    "    임베딩 품질 평가 (같은 평점의 리뷰가 서로 더 유사한지 확인)\n",
    "    \n",
    "    Args:\n",
    "        review_embeddings (np.ndarray): 리뷰 임베딩 배열\n",
    "        ratings (pd.Series): 평점\n",
    "        n_samples (int): 평가할 샘플 수\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (같은 평점 리뷰 간 평균 유사도, 다른 평점 리뷰 간 평균 유사도)\n",
    "    \"\"\"\n",
    "    if len(review_embeddings) > n_samples:\n",
    "        indices = np.random.choice(len(review_embeddings), n_samples, replace=False)\n",
    "        embeddings_sample = review_embeddings[indices]\n",
    "        ratings_sample = ratings.iloc[indices].reset_index(drop=True)\n",
    "    else:\n",
    "        embeddings_sample = review_embeddings\n",
    "        ratings_sample = ratings.reset_index(drop=True)\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    similarities = cosine_similarity(embeddings_sample)\n",
    "    \n",
    "    # 같은 평점 및 다른 평점 리뷰 쌍 찾기\n",
    "    same_rating_sims = []\n",
    "    diff_rating_sims = []\n",
    "    \n",
    "    for i in range(len(embeddings_sample)):\n",
    "        for j in range(i+1, len(embeddings_sample)):\n",
    "            if ratings_sample.iloc[i] == ratings_sample.iloc[j]:\n",
    "                same_rating_sims.append(similarities[i, j])\n",
    "            else:\n",
    "                diff_rating_sims.append(similarities[i, j])\n",
    "    \n",
    "    same_rating_mean = np.mean(same_rating_sims)\n",
    "    diff_rating_mean = np.mean(diff_rating_sims)\n",
    "    \n",
    "    print(f\"같은 평점 리뷰 간 평균 유사도: {same_rating_mean:.4f}\")\n",
    "    print(f\"다른 평점 리뷰 간 평균 유사도: {diff_rating_mean:.4f}\")\n",
    "    print(f\"차이 (같은 평점 - 다른 평점): {same_rating_mean - diff_rating_mean:.4f}\")\n",
    "    \n",
    "    return same_rating_mean, diff_rating_mean\n",
    "\n",
    "# 임베딩 품질 평가\n",
    "evaluate_embedding_quality(train_review_embeddings_reduced, train_df['rating'], n_samples=2000)\n",
    "\n",
    "# 동일 비즈니스 리뷰 간 유사도 분석\n",
    "def evaluate_business_coherence(review_embeddings, business_ids, n_samples=500):\n",
    "    \"\"\"\n",
    "    동일 비즈니스 리뷰 간 유사도 분석\n",
    "    \n",
    "    Args:\n",
    "        review_embeddings (np.ndarray): 리뷰 임베딩 배열\n",
    "        business_ids (pd.Series): 비즈니스 ID\n",
    "        n_samples (int): 평가할 샘플 수\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (동일 비즈니스 리뷰 간 평균 유사도, 다른 비즈니스 리뷰 간 평균 유사도)\n",
    "    \"\"\"\n",
    "    # 자주 리뷰된 비즈니스 선택\n",
    "    top_businesses = business_ids.value_counts().head(10).index.tolist()\n",
    "    \n",
    "    # 샘플링\n",
    "    mask = business_ids.isin(top_businesses)\n",
    "    if sum(mask) > n_samples:\n",
    "        indices = np.random.choice(np.where(mask)[0], n_samples, replace=False)\n",
    "    else:\n",
    "        indices = np.where(mask)[0]\n",
    "    \n",
    "    embeddings_sample = review_embeddings[indices]\n",
    "    business_sample = business_ids.iloc[indices].reset_index(drop=True)\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    similarities = cosine_similarity(embeddings_sample)\n",
    "    \n",
    "    # 동일 비즈니스 및 다른 비즈니스 리뷰 쌍 찾기\n",
    "    same_business_sims = []\n",
    "    diff_business_sims = []\n",
    "    \n",
    "    for i in range(len(embeddings_sample)):\n",
    "        for j in range(i+1, len(embeddings_sample)):\n",
    "            if business_sample.iloc[i] == business_sample.iloc[j]:\n",
    "                same_business_sims.append(similarities[i, j])\n",
    "            else:\n",
    "                diff_business_sims.append(similarities[i, j])\n",
    "    \n",
    "    same_business_mean = np.mean(same_business_sims)\n",
    "    diff_business_mean = np.mean(diff_business_sims)\n",
    "    \n",
    "    print(f\"동일 비즈니스 리뷰 간 평균 유사도: {same_business_mean:.4f}\")\n",
    "    print(f\"다른 비즈니스 리뷰 간 평균 유사도: {diff_business_mean:.4f}\")\n",
    "    print(f\"차이 (동일 비즈니스 - 다른 비즈니스): {same_business_mean - diff_business_mean:.4f}\")\n",
    "    \n",
    "    return same_business_mean, diff_business_mean\n",
    "\n",
    "# 비즈니스 리뷰 일관성 평가\n",
    "evaluate_business_coherence(train_review_embeddings_reduced, train_df['item_idx'], n_samples=1000)\n",
    "\n",
    "print(\"\\n텍스트 임베딩 생성 및 분석 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
